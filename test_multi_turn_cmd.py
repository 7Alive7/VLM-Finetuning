#!/usr/bin/python3.10.14
# -*- coding: utf-8 -*-

import torch
from transformers import AutoTokenizer, AutoProcessor, AutoModelForCausalLM
from config import VLMConfig
from train_with_attention_mask import VLM
from PIL import Image


# åˆå§‹åŒ–é…ç½®å’Œæ¨¡å‹
config = VLMConfig()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# æ³¨å†Œ VLM ç”¨äºè‡ªåŠ¨åŠ è½½
from transformers import AutoConfig
AutoConfig.register("vlm_model", VLMConfig)
AutoModelForCausalLM.register(VLMConfig, VLM)

# åŠ è½½æ¨¡å‹å’Œå·¥å…·
model_path = ""  # å¯æ”¹ä¸º SFT æ¨¡å‹è·¯å¾„
model = AutoModelForCausalLM.from_pretrained(model_path).to(device)
tokenizer = AutoTokenizer.from_pretrained(config.llm_path)
processor = AutoProcessor.from_pretrained(config.vision_model_path)
qwen_model = AutoModelForCausalLM.from_pretrained(config.llm_path)
model.llm.lm_head.load_state_dict(qwen_model.lm_head.state_dict())
model.eval()
# è¾“å…¥å›¾åƒè·¯å¾„
image_path = input("è¯·è¾“å…¥å›¾åƒè·¯å¾„ï¼ˆå¯ç›´æ¥æ‹–æ‹½å›¾ç‰‡è¿›æ¥ï¼‰:\n>>> ").strip().strip("'")
image = Image.open(image_path).convert("RGB")
pixel_values = processor(images=image, return_tensors="pt")["pixel_values"].to(device)

# å¯¹è¯å†å²
history = [{"role": "system", "content": "You are a helpful assistant."}]
first_round = True

print(f'LLM load path: {model_path}')

print("\nå¼€å§‹å¯¹è¯ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰")
while True:
    user_input = input("ä½ ï¼š")
    if user_input.strip().lower() in ["exit", "quit"]:
        break

    # åœ¨ç¬¬ä¸€è½®å¯¹è¯ä¸­ï¼Œç¡®ä¿åœ¨ç”¨æˆ·è¾“å…¥ç»“å°¾è¿½åŠ  "\n<image>"
    if first_round:
        if not user_input.endswith("\n<image>"):
            user_input = user_input + "\n<image>"
            current_pixel = pixel_values
        first_round = False
        cached_pixel_values = pixel_values
    else:
        current_pixel = cached_pixel_values

    # æ„å»º Prompt + æ’å…¥ image pad
    history.append({"role": "user", "content": user_input})
    prompt_text = tokenizer.apply_chat_template(
        history,
        tokenize=False,
        add_generation_prompt=True
    ).replace("<image>", "<|image_pad|>" * config.image_pad_num)

    input_ids = tokenizer(prompt_text, return_tensors="pt").input_ids.to(device)
    attention_mask = torch.ones_like(input_ids)

    with torch.no_grad():
        output_ids = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            pixel_values=current_pixel,
            max_new_tokens=256,
            temperature=0.7
        )

    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    print(f"ğŸ¤–ï¼š{answer}\n")

    history.append({"role": "assistant", "content": answer})
